{
 "metadata": {},
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "PyIBP\n",
      "\n",
      "Implements fast Gibbs sampling for the linear-Gaussian\n",
      "infinite latent feature model (IBP).\n",
      "\n",
      "Copyright (C) 2009 David Andrzejewski (andrzeje@cs.wisc.edu)\n",
      "\n",
      "This program is free software: you can redistribute it and/or modify\n",
      "it under the terms of the GNU General Public License as published by\n",
      "the Free Software Foundation, either version 3 of the License, or\n",
      "(at your option) any later version.\n",
      "\n",
      "This program is distributed in the hope that it will be useful,\n",
      "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      "GNU General Public License for more details.\n",
      "\n",
      "You should have received a copy of the GNU General Public License\n",
      "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
      "\"\"\"\n",
      "\n",
      "import numpy as NP\n",
      "import numpy.random as NR\n",
      "import scipy.stats as SPST\n",
      "import scipy.special as SPS\n",
      "\n",
      "# We will be taking log(0) = -Inf, so turn off this warning\n",
      "NP.seterr(divide='ignore')\n",
      "\n",
      "class PyIBP(object):\n",
      "    \"\"\"\n",
      "    Implements fast Gibbs sampling for the linear-Gaussian\n",
      "    infinite latent feature model (IBP)\n",
      "    \"\"\"\n",
      "\n",
      "    #\n",
      "    # Initialization methods\n",
      "    #\n",
      "\n",
      "    def __init__(self,data,alpha,sigma_x,sigma_a,\n",
      "                 missing=None,useV=False,initZV=None):\n",
      "        \"\"\" \n",
      "        data = NxD NumPy data matrix (should be centered)\n",
      "\n",
      "        alpha = Fixed IBP hyperparam for OR (init,a,b) tuple where\n",
      "        (a,b) are Gamma hyperprior shape and rate/inverse scale\n",
      "        sigma_x = Fixed noise std OR (init,a,b) tuple (same as alpha)\n",
      "        sigma_a = Fixed weight std OR (init,a,b) tuple (same as alpha)\n",
      "        \n",
      "        OPTIONAL ARGS\n",
      "        missing = boolean/binary 'missing data' mask (1=missing entry)\n",
      "        useV = Are we using real-valued latent features? (default binary)\n",
      "        initZV = Optional initial state for the latent         \n",
      "        \"\"\"\n",
      "        # Data matrix\n",
      "        self.X = data\n",
      "        (self.N,self.D) = data.shape\n",
      "        # IBP hyperparameter\n",
      "        if(type(alpha) == tuple):\n",
      "            (self.alpha,self.alpha_a,self.alpha_b) = alpha\n",
      "        else:\n",
      "            (self.alpha,self.alpha_a,self.alpha_b) = (alpha,None,None)\n",
      "        # Noise variance hyperparameter\n",
      "        if(type(sigma_x) == tuple):\n",
      "            (self.sigma_x,self.sigma_xa,self.sigma_xb) = sigma_x\n",
      "        else:\n",
      "            (self.sigma_x,self.sigma_xa,self.sigma_xb) = (sigma_x,None,None)\n",
      "        # Weight variance hyperparameter\n",
      "        if(type(sigma_a) == tuple):\n",
      "            (self.sigma_a,self.sigma_aa,self.sigma_ab) = sigma_a\n",
      "        else:\n",
      "            (self.sigma_a,self.sigma_aa,self.sigma_ab) = (sigma_a,None,None)\n",
      "        # Are we using weighted latent features?\n",
      "        self.useV = useV                            \n",
      "        # Do we have user-supplied initial latent feature values?\n",
      "        if(initZV == None):\n",
      "            # Initialze Z from IBP(alpha)\n",
      "            self.initZ()\n",
      "            # Initialize V from N(0,1) if necessary\n",
      "            if(self.useV):\n",
      "                self.initV()\n",
      "        else:\n",
      "            self.ZV = initZV\n",
      "            self.K = self.ZV.shape[1]\n",
      "            self.m = (self.ZV != 0).astype(NP.int).sum(axis=0)\n",
      "        # Sample missing data entries if necessary\n",
      "        self.missing = missing\n",
      "        if(missing != None):\n",
      "            self.sampleX()\n",
      "\n",
      "    def initV(self):\n",
      "        \"\"\" Init latent feature weights V accoring to N(0,1) \"\"\"        \n",
      "        for (i,k) in zip(*self.ZV.nonzero()):\n",
      "            self.ZV[i,k] = NR.normal(0,1)\n",
      "\n",
      "    def initZ(self):\n",
      "        \"\"\" Init latent features Z according to IBP(alpha) \"\"\"\n",
      "        Z = NP.ones((0,0))\n",
      "        for i in range(1,self.N+1):\n",
      "            # Sample existing features\n",
      "            zi = (NR.uniform(0,1,(1,Z.shape[1])) <\n",
      "                  (Z.sum(axis=0).astype(NP.float) / i))\n",
      "            # Sample new features\n",
      "            knew = SPST.poisson.rvs(self.alpha / i)\n",
      "            zi = NP.hstack((zi,NP.ones((1,knew))))\n",
      "            # Add to Z matrix\n",
      "            Z = NP.hstack((Z,NP.zeros((Z.shape[0],knew))))\n",
      "            Z = NP.vstack((Z,zi))\n",
      "        self.ZV = Z\n",
      "        self.K = self.ZV.shape[1]\n",
      "        # Calculate initial feature counts\n",
      "        self.m = (self.ZV != 0).astype(NP.int).sum(axis=0)\n",
      "\n",
      "    #\n",
      "    # Convenient external methods\n",
      "    #\n",
      "\n",
      "    def fullSample(self):\n",
      "        \"\"\" Do all applicable samples \"\"\"\n",
      "        self.sampleZ()\n",
      "        if(self.missing != None):\n",
      "            self.sampleX()\n",
      "        if(self.alpha_a != None):\n",
      "            self.sampleAlpha()\n",
      "        if(self.sigma_xa != None):\n",
      "            self.sampleSigma()\n",
      "\n",
      "    def logLike(self):\n",
      "        \"\"\"\n",
      "        Calculate log-likelihood P(X,Z)\n",
      "        (or P(X,Z,V) if applicable)\n",
      "        \"\"\"\n",
      "        liketerm = self.logPX(self.calcM(self.ZV),self.ZV)\n",
      "        ibpterm = self.logIBP()    \n",
      "        if(self.useV):\n",
      "            vterm = self.logPV()\n",
      "            return liketerm+ibpterm+vterm\n",
      "        else:\n",
      "            return liketerm+ibpterm\n",
      "\n",
      "    def weights(self):\n",
      "        \"\"\" Return E[A|X,Z] \"\"\"\n",
      "        return self.postA(self.X,self.ZV)[0]\n",
      "\n",
      "    #\n",
      "    # Actual sampling methods\n",
      "    #\n",
      "\n",
      "    def sampleV(self,k,meanA,covarA,xi,zi):\n",
      "        \"\"\" Slice sampling for feature weight V \"\"\"\n",
      "        oldv = zi[0,k]\n",
      "        # Log-posterior of current value\n",
      "        curlp = self.vLogPost(k,zi[0,k],meanA,covarA,xi,zi)\n",
      "        # Vertically sample beneath this value\n",
      "        curval = self.logUnif(curlp)\n",
      "        # Initial sample from horizontal slice\n",
      "        (left,right) = self.makeInterval(curval,k,zi[0,k],\n",
      "                                         meanA,covarA,xi,zi)\n",
      "        newv = NR.uniform(left,right)\n",
      "        newval = self.vLogPost(k,newv,meanA,covarA,xi,zi)\n",
      "        # Repeat until valid sample obtained\n",
      "        while(newval <= curval):\n",
      "            if(newv < zi[0,k]):\n",
      "                left = newv\n",
      "            else:\n",
      "                right = newv\n",
      "            newv = NR.uniform(left,right)\n",
      "            newval = self.vLogPost(k,newv,meanA,covarA,xi,zi)\n",
      "        return newv        \n",
      "\n",
      "    def makeInterval(self,u,k,v,meanA,covarA,xi,zi):\n",
      "        \"\"\" Get horizontal slice sampling interval \"\"\"\n",
      "        w = .25\n",
      "        (left,right) = (v-w,v+w)\n",
      "        (leftval,rightval) = (self.vLogPost(k,left,meanA,covarA,xi,zi),\n",
      "                              self.vLogPost(k,right,meanA,covarA,xi,zi))\n",
      "        while(leftval > u):\n",
      "            left -= w \n",
      "            leftval = self.vLogPost(k,left,meanA,covarA,xi,zi)\n",
      "        while(rightval > u):\n",
      "            right += w\n",
      "            rightval = self.vLogPost(k,right,meanA,covarA,xi,zi) \n",
      "        return (left,right)\n",
      "\n",
      "    def vLogPost(self,k,v,meanA,covarA,xi,zi):\n",
      "        \"\"\" For a given V, calculate the log-posterior \"\"\"        \n",
      "        oldv = zi[0,k]\n",
      "        zi[0,k] = v\n",
      "        (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "        logprior = -0.5*(v**2) - 0.5*NP.log(2*NP.pi)\n",
      "        loglike = self.logPxi(meanLike,covarLike,xi)\n",
      "        # Restore previous value and return result\n",
      "        zi[0,k] = oldv\n",
      "        return logprior + loglike\n",
      "\n",
      "    def sampleSigma(self):\n",
      "        \"\"\" Sample feature/noise variances \"\"\"\n",
      "        # Posterior over feature weights A\n",
      "        (meanA,covarA) = self.postA(self.X,self.ZV)\n",
      "        # sigma_x\n",
      "        vars = NP.dot(self.ZV,NP.dot(covarA,self.ZV.T)).diagonal()\n",
      "        var_x = (NP.power(self.X - NP.dot(self.ZV,meanA), 2)).sum()\n",
      "        var_x += self.D * vars.sum()\n",
      "        n = float(self.N * self.D)\n",
      "        postShape = self.sigma_xa + n/2\n",
      "        postScale = float(1) / (self.sigma_xb + var_x/2)\n",
      "        tau_x = SPST.gamma.rvs(postShape,scale=postScale)\n",
      "        self.sigma_x = NP.sqrt(float(1)/tau_x)\n",
      "        # sigma_a\n",
      "        var_a = covarA.trace() * self.D + NP.power(meanA,2).sum()\n",
      "        n = float(self.K * self.D)\n",
      "        postShape = self.sigma_aa + n/2\n",
      "        postScale = float(1) / (self.sigma_ab + var_a/2)\n",
      "        tau_a = SPST.gamma.rvs(postShape,scale=postScale)        \n",
      "        self.sigma_a = NP.sqrt(float(1)/tau_a)\n",
      "        if(self.sigma_a > 100):\n",
      "            pdb.set_trace()\n",
      "        \n",
      "    def sampleAlpha(self):\n",
      "        \"\"\" Sample alpha from conjugate posterior \"\"\"\n",
      "        postShape = self.alpha_a + self.m.sum()\n",
      "        postScale = float(1) / (self.alpha_b + self.N)\n",
      "        self.alpha = SPST.gamma.rvs(postShape,scale=postScale)\n",
      "\n",
      "    def sampleX(self):\n",
      "        \"\"\" Take single sample missing data entries in X \"\"\"\n",
      "        # Calculate posterior mean/covar --> info\n",
      "        (meanA,covarA) = self.postA(self.X,self.ZV) \n",
      "        (infoA,hA) = self.toInfo(meanA,covarA)\n",
      "        # Find missing observations\n",
      "        xis = NP.nonzero(self.missing.max(axis=1))[0]\n",
      "        for i in xis:\n",
      "            # Get (z,x) for this data point\n",
      "            (zi,xi) = (NP.reshape(self.ZV[i,:],(1,self.K)),\n",
      "                       NP.reshape(self.X[i,:],(1,self.D)))\n",
      "            # Remove this observation\n",
      "            infoA_i = self.updateInfo(infoA,zi,-1)\n",
      "            hA_i = self.updateH(hA,zi,xi,-1)\n",
      "            # Convert back to mean/covar\n",
      "            (meanA_i,covarA_i) = self.fromInfo(infoA_i,hA_i)            \n",
      "            # Resample xi\n",
      "            (meanXi,covarXi) = self.likeXi(zi,meanA_i,covarA_i)                \n",
      "            newxi = NR.normal(meanXi,NP.sqrt(covarXi))\n",
      "            # Replace missing features\n",
      "            ks = NP.nonzero(self.missing[i,:])[0]\n",
      "            self.X[i,ks] = newxi[0][ks]\n",
      "\n",
      "    def sampleZ(self):\n",
      "        \"\"\" Take single sample of latent features Z \"\"\"\n",
      "        # for each data point\n",
      "        order = NR.permutation(self.N)\n",
      "        for (ctr,i) in enumerate(order):\n",
      "            # Initially, and later occasionally,\n",
      "            # re-cacluate information directly\n",
      "            if(ctr % 5 == 0):\n",
      "                try:\n",
      "                    (meanA,covarA) = self.postA(self.X,self.ZV)\n",
      "                    (infoA,hA) = self.toInfo(meanA,covarA)\n",
      "                except Exception,e:\n",
      "                    pdb.set_trace()                \n",
      "            # Get (z,x) for this data point\n",
      "            (zi,xi) = (NP.reshape(self.ZV[i,:],(1,self.K)),\n",
      "                       NP.reshape(self.X[i,:],(1,self.D)))\n",
      "            # Remove this point from information\n",
      "            infoA = self.updateInfo(infoA,zi,-1)\n",
      "            hA = self.updateH(hA,zi,xi,-1)\n",
      "            # Convert back to mean/covar\n",
      "            (meanA,covarA) = self.fromInfo(infoA,hA)            \n",
      "            # Remove this data point from feature cts\n",
      "            newcts = self.m - (self.ZV[i,:] != 0).astype(NP.int)\n",
      "            # Log collapsed Beta-Bernoulli terms\n",
      "            lpz1 = NP.log(newcts)\n",
      "            lpz0 = NP.log(self.N - newcts)\n",
      "            # Find all singleton features\n",
      "            singletons = [ki for ki in range(self.K) if\n",
      "                          self.ZV[i,ki] != 0 and self.m[ki] == 1]\n",
      "            nonsingletons = [ki for ki in range(self.K) if\n",
      "                             ki not in singletons]\n",
      "            # Sample for each non-singleton feature\n",
      "            #\n",
      "            for k in nonsingletons:\n",
      "                oldz = zi[0,k]\n",
      "                # z=0 case\n",
      "                lp0 = lpz0[k]\n",
      "                zi[0,k] = 0\n",
      "                (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "                lp0 += self.logPxi(meanLike,covarLike,xi)\n",
      "                # z=1 case\n",
      "                lp1 = lpz1[k]\n",
      "                if(self.useV):\n",
      "                    if(oldz != 0):\n",
      "                        # Use current V value\n",
      "                        zi[0,k] = oldz\n",
      "                        (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "                        lp1 += self.logPxi(meanLike,covarLike,xi)\n",
      "                    else:\n",
      "                        # Sample V values from the prior to \n",
      "                        # numerically collapse/integrate\n",
      "                        nvs = 5\n",
      "                        lps = NP.zeros((nvs,))\n",
      "                        for vs in range(nvs):\n",
      "                            zi[0,k] = NR.normal(0,1)\n",
      "                            (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "                            lps[vs] = self.logPxi(meanLike,covarLike,xi)\n",
      "                        lp1 += lps.mean()\n",
      "                else:\n",
      "                    zi[0,k] = 1\n",
      "                    (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "                    lp1 += self.logPxi(meanLike,covarLike,xi)\n",
      "                # Sample Z, update feature counts\n",
      "                if(not self.logBern(lp0,lp1)):\n",
      "                    zi[0,k] = 0\n",
      "                    if(oldz != 0):\n",
      "                        self.m[k] -= 1\n",
      "                else:\n",
      "                    if(oldz == 0):\n",
      "                        self.m[k] += 1\n",
      "                    if(self.useV):\n",
      "                        # Slice sample V from posterior if necessary\n",
      "                        zi[0,k] = self.sampleV(k,meanA,covarA,xi,zi)\n",
      "                if(self.m[k] != ((self.ZV[:,k] != 0 ).astype(NP.int)).sum()):\n",
      "                    pdb.set_trace()\n",
      "                if(self.m[k]>self.N):\n",
      "                    pdb.set_trace()\n",
      "            #\n",
      "            # Sample singleton/new features using the\n",
      "            # Metropolis-Hastings step described in Meeds et al\n",
      "            #\n",
      "            kold = len(singletons)\n",
      "            # Sample from the Metropolis proposal\n",
      "            knew = SPST.poisson.rvs(self.alpha / self.N)\n",
      "            if(self.useV):\n",
      "                vnew = NP.array([NR.normal(0,1) for k in range(knew)])\n",
      "            # Net difference in number of singleton features\n",
      "            netdiff = knew - kold\n",
      "            # Contribution of singleton features to variance in x\n",
      "            if(self.useV):\n",
      "                prevcontrib = NP.power(zi[0,singletons],2).sum()\n",
      "                newcontrib = NP.power(vnew,2).sum()\n",
      "                weightdiff = newcontrib - prevcontrib\n",
      "            else:\n",
      "                weightdiff = knew - kold\n",
      "            # Calculate the loglikelihoods\n",
      "            (meanLike,covarLike) = self.likeXi(zi,meanA,covarA)\n",
      "            lpold = self.logPxi(meanLike,covarLike,xi)\n",
      "            lpnew = self.logPxi(meanLike,\n",
      "                                covarLike + weightdiff*self.sigma_a**2,\n",
      "                                xi)\n",
      "            lpaccept = min(0.0, lpnew-lpold)\n",
      "            lpreject = NP.log(max(1.0 - NP.exp(lpaccept), 1e-100))\n",
      "            if(self.logBern(lpreject,lpaccept)):\n",
      "                # Accept the Metropolis-Hastings proposal\n",
      "                if(netdiff > 0):\n",
      "                    # We're adding features, update ZV\n",
      "                    self.ZV = NP.append(self.ZV,NP.zeros((self.N,netdiff)),1)\n",
      "                    if(self.useV):\n",
      "                        prevNumSingletons = len(singletons)\n",
      "                        self.ZV[i,singletons] = vnew[:prevNumSingletons]\n",
      "                        self.ZV[i,self.K:] = vnew[prevNumSingletons:]\n",
      "                    else:\n",
      "                        self.ZV[i,self.K:] = 1\n",
      "                    # Update feature counts m\n",
      "                    self.m = NP.append(self.m,NP.ones(netdiff),0)\n",
      "                    # Append information matrix with 1/sigmaa^2 diag\n",
      "                    infoA = NP.vstack((infoA,NP.zeros((netdiff,self.K))))\n",
      "                    infoA = NP.hstack((infoA,\n",
      "                                       NP.zeros((netdiff+self.K,netdiff))))\n",
      "                    infoappend = (1 / self.sigma_a**2) * NP.eye(netdiff)\n",
      "                    infoA[self.K:(self.K+netdiff),\n",
      "                          self.K:(self.K+netdiff)] = infoappend\n",
      "                    # only need to resize (expand) hA\n",
      "                    hA = NP.vstack((hA,NP.zeros((netdiff,self.D))))\n",
      "                    # Note that the other effects of new latent features \n",
      "                    # on (infoA,hA) (ie, the zi terms) will be counted when \n",
      "                    # this zi is added back in                    \n",
      "                    self.K += netdiff\n",
      "                elif(netdiff < 0):\n",
      "                    # We're removing features, update ZV\n",
      "                    if(self.useV):\n",
      "                        self.ZV[i,singletons[(-1*netdiff):]] = vnew\n",
      "                    dead = [ki for ki in singletons[:(-1*netdiff)]]\n",
      "                    self.K -= len(dead)\n",
      "                    self.ZV = NP.delete(self.ZV,dead,axis=1)\n",
      "                    self.m = NP.delete(self.m,dead)\n",
      "                    # Easy to do this b/c these features did not\n",
      "                    # occur in any other data points anyways...\n",
      "                    infoA = NP.delete(infoA,dead,axis=0)\n",
      "                    infoA = NP.delete(infoA,dead,axis=1)\n",
      "                    hA = NP.delete(hA,dead,axis=0)                    \n",
      "                else:\n",
      "                    # net difference is actually zero, just replace\n",
      "                    # the latent weights of existing singletons\n",
      "                    # (if applicable)\n",
      "                    if(self.useV):\n",
      "                        self.ZV[i,singletons] = vnew\n",
      "            # Add this point back into information\n",
      "            #\n",
      "            zi = NP.reshape(self.ZV[i,:],(1,self.K))\n",
      "            infoA = self.updateInfo(infoA,zi,1)\n",
      "            hA = self.updateH(hA,zi,xi,1)\n",
      "\n",
      "    #\n",
      "    # Output and reporting\n",
      "    # \n",
      "\n",
      "    def sampleReport(self,sampleidx):\n",
      "        \"\"\" Print IBP sample status \"\"\"\n",
      "        print 'iter %d' % sampleidx\n",
      "        print '\\tcollapsed loglike = %f' % self.logLike()\n",
      "        print '\\tK = %d' % self.K\n",
      "        print '\\talpha = %f' % self.alpha\n",
      "        print '\\tsigma_x = %f' % self.sigma_x\n",
      "        print '\\tsigma_a = %f' % self.sigma_a\n",
      "                \n",
      "    def weightReport(self,trueWeights=None,round=False):\n",
      "        \"\"\" Print learned weights (vs ground truth if available) \"\"\"\n",
      "        if(trueWeights != None):\n",
      "            print '\\nTrue weights (A)'\n",
      "            print str(trueWeights)\n",
      "        print '\\nLearned weights (A)'\n",
      "        # Print rounded or actual weights?\n",
      "        if(round):\n",
      "            print str(self.weights().astype(NP.int))\n",
      "        else:\n",
      "            print NP.array_str(self.weights(),precision=2,suppress_small=True)\n",
      "        print ''\n",
      "        # Print V matrix if applicable\n",
      "        if(self.useV):\n",
      "            print '\\nLatent feature weights (V)'\n",
      "            print NP.array_str(self.ZV,precision=2)\n",
      "            print ''\n",
      "        # Print 'popularity' of latent features\n",
      "        print '\\nLatent feature counts (m)'\n",
      "        print NP.array_str(self.m)\n",
      "\n",
      "    #\n",
      "    # Bookkeeping and calculation methods\n",
      "    #\n",
      "\n",
      "    def logPV(self):\n",
      "        \"\"\" Log-likelihood of real-valued latent features V \"\"\"\n",
      "        lpv = -0.5*NP.power(self.ZV,2).sum()\n",
      "        return lpv - len(self.ZV.nonzero()[0]) * 0.5 * NP.log(2*NP.pi)\n",
      "\n",
      "    def logIBP(self):\n",
      "        \"\"\" Calculate IBP prior contribution log P(Z|alpha) \"\"\"\n",
      "        (N,K) = self.ZV.shape\n",
      "        # Need to find all unique K 'histories'\n",
      "        Z = (self.ZV != 0).astype(NP.int)\n",
      "        Khs = {}\n",
      "        for k in range(K):\n",
      "            history = tuple(Z[:,k])\n",
      "            Khs[history] = Khs.get(history,0) + 1\n",
      "        logp = 0\n",
      "        logp += self.K * NP.log(self.alpha)\n",
      "        for Kh in Khs.values():\n",
      "            logp -= self.logFact(Kh)\n",
      "        logp -= self.alpha * sum([float(1) / i for i in range(1,N+1)])\n",
      "        for k in range(K):\n",
      "            logp += self.logFact(N-self.m[k]) + self.logFact(self.m[k]-1)\n",
      "            logp -= self.logFact(N)        \n",
      "        if(logp==float('inf')):\n",
      "            pdb.set_trace()\n",
      "        return logp    \n",
      "    \n",
      "    def postA(self,X,Z):\n",
      "        \"\"\" Mean/covar of posterior over weights A \"\"\"\n",
      "        M = self.calcM(Z)\n",
      "        meanA = NP.dot(M,NP.dot(Z.T,X))\n",
      "        covarA = self.sigma_x**2 * self.calcM(Z)\n",
      "        return (meanA,covarA)\n",
      "\n",
      "    def calcM(self,Z):\n",
      "        \"\"\" Calculate M = (Z' * Z - (sigmax^2) / (sigmaa^2) * I)^-1 \"\"\"\n",
      "        return NP.linalg.inv(NP.dot(Z.T,Z) + (self.sigma_x**2) \n",
      "                             / (self.sigma_a**2) * NP.eye(self.K))\n",
      "\n",
      "    def logPX(self,M,Z):\n",
      "        \"\"\" Calculate collapsed log likelihood of data\"\"\"\n",
      "        lp = -0.5 * self.N * self.D * NP.log(2*NP.pi)\n",
      "        lp -= (self.N - self.K) * self.D * NP.log(self.sigma_x)\n",
      "        lp -= self.K * self.D * NP.log(self.sigma_a)\n",
      "        lp -= 0.5 * self.D * NP.log(NP.linalg.det(NP.linalg.inv(M)))\n",
      "        iminzmz = NP.eye(self.N) - NP.dot(Z,NP.dot(M,Z.T))\n",
      "        lp -= (0.5 / (self.sigma_x**2)) * NP.trace(\n",
      "            NP.dot(self.X.T,NP.dot(iminzmz,self.X)))\n",
      "        return lp\n",
      "\n",
      "    def likeXi(self,zi,meanA,covarA):\n",
      "        \"\"\" Mean/covar of xi given posterior over A \"\"\"\n",
      "        meanXi = NP.dot(zi,meanA)\n",
      "        covarXi = NP.dot(zi,NP.dot(covarA,zi.T)) + self.sigma_x**2\n",
      "        return (meanXi,covarXi)\n",
      "\n",
      "    def updateInfo(self,infoA,zi,addrm):\n",
      "        \"\"\" Add/remove data i to/from information \"\"\"\n",
      "        return infoA + addrm * ((1/self.sigma_x**2) * NP.dot(zi.T,zi))\n",
      "\n",
      "    def updateH(self,hA,zi,xi,addrm):\n",
      "        \"\"\" Add/remove data i to/from h\"\"\"\n",
      "        return hA + addrm * ((1/self.sigma_x**2) * NP.dot(zi.T,xi))\n",
      "    \n",
      "    #\n",
      "    # Pure functions (these don't use state or additional params)\n",
      "    #\n",
      "\n",
      "    @staticmethod\n",
      "    def logFact(n):\n",
      "        return SPS.gammaln(n+1)\n",
      "    \n",
      "    @staticmethod\n",
      "    def fromInfo(infoA,hA):\n",
      "        \"\"\" Calculate mean/covar from information \"\"\"\n",
      "        covarA = NP.linalg.inv(infoA)\n",
      "        meanA = NP.dot(covarA,hA)\n",
      "        return (meanA,covarA)\n",
      "\n",
      "    @staticmethod\n",
      "    def toInfo(meanA,covarA):\n",
      "        \"\"\" Calculate information from mean/covar \"\"\"\n",
      "        infoA = NP.linalg.inv(covarA)\n",
      "        hA = NP.dot(infoA,meanA)        \n",
      "        return (infoA,hA)\n",
      "\n",
      "    @staticmethod\n",
      "    def logUnif(v):\n",
      "        \"\"\" \n",
      "        Sample uniformly from [0, exp(v)] in the log-domain\n",
      "        (derive via transform f(x)=log(x) and some calculus...)\n",
      "        \"\"\"\n",
      "        return v + NP.log(NR.uniform(0,1))\n",
      "\n",
      "    @staticmethod\n",
      "    def logBern(lp0,lp1):\n",
      "        \"\"\" Bernoulli sample given log(p0) and log(p1) \"\"\"\n",
      "        p1 = 1 / (1+NP.exp(lp0-lp1))\n",
      "        return (p1 > NR.uniform(0,1))\n",
      "\n",
      "    @staticmethod\n",
      "    def logPxi(meanLike,covarLike,xi):\n",
      "        \"\"\"\n",
      "        Calculate log-likelihood of a single xi, given its\n",
      "        mean/covar after collapsing P(A | X_{-i}, Z)\n",
      "        \"\"\"\n",
      "        D = float(xi.shape[1])\n",
      "        ll = -(D / 2) * NP.log(covarLike)\n",
      "        ll -= (1 / (2*covarLike)) * NP.power(xi-meanLike,2).sum()\n",
      "        return ll\n",
      "    \n",
      "    @staticmethod\n",
      "    def centerData(data):\n",
      "        return data - PyIBP.featMeans(data)\n",
      "\n",
      "    @staticmethod\n",
      "    def featMeans(data,missing=None):\n",
      "        \"\"\" Replace all columns (features) with their means \"\"\"\n",
      "        (N,D) = data.shape\n",
      "        if(missing == None):\n",
      "            return NP.tile(data.mean(axis=0),(N,1))\n",
      "        else:        \n",
      "            # Sanity check on 'missing' mask\n",
      "            # (ensure no totally missing data or features)\n",
      "            assert(all(missing.sum(axis=0) < N) and\n",
      "                   all(missing.sum(axis=1) < D))\n",
      "            # Calculate column means without using the missing data\n",
      "            censored = data * (NP.ones((N,D)) - missing)\n",
      "            censoredmeans = censored.sum(axis=0) / (N-missing.sum(axis=0))\n",
      "            return NP.tile(censoredmeans,(N,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}