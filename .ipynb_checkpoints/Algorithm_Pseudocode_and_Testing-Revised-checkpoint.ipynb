{
 "metadata": {
  "name": "",
  "signature": "sha256:b98e4f5d004953c9b7d74ae598a4b661769ae5e1d0b9676db6a8a7b71a934603"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Algorithm Outline -- Indian Buffet Process (IBP) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Indian Buffet Process (IBP) Steps\n",
      "\n",
      "- Step 1: First customer takes a Poisson($\\alpha$) of dishes\n",
      "- Step 2: The $i$th customer takes dish $k$ with probability $\\frac{m_k}{i}$, where $m_k$ is the number of previous customers who sampled that dish\n",
      "- Step 3: The $i$th customer tries a Poisson($\\frac{\\alpha}{i}$) number of new dishes\n",
      "- Storage: Binary matrix $\\mathbf{Z}$ with $N$ rows and infinite columns, where $z_{ik}$ = 1 if customer $i$ sampled the dish $k$\n",
      "- Probability: $P(z_{ik}=1 | \\mathbf{z_{-i,k}}) = \\dfrac{m_{-i,k}+\\dfrac{\\alpha}{K}}{N+\\dfrac{\\alpha}{K}} \\rightarrow \\dfrac{m_{-i,k}}{N}$ as $K \\rightarrow \\infty$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Algorithm Application -- Linear-Gaussian Binary Latent Feature Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Variables in Data\n",
      "\n",
      "$N = 100$ is the number of images (customers, objects)\n",
      "\n",
      "$D = 6 \\times 6 = 36$ is the length of vectors (dishes, features) for each image\n",
      "\n",
      "$K = 4$ is the number of basis images (latent or underlying variables)\n",
      "\n",
      "Each object $i$ has a $D$-dimensional vector of properties, named $x_i$\n",
      "\n",
      "- Generate images $X$ with the $K$ basis images, indicating which bases are used (each with probability 0.5)\n",
      "- Add white noises $\\text{Normal}(0,\\sigma_X^2 = 0.5)$ to these images\n",
      "\n",
      "### Variables in Transformation\n",
      "\n",
      "$x_i \\sim \\text{Normal}(\\mathbf{z_i} \\mathbf{A}, \\Sigma_X = \\sigma_X^2\\mathbf{I})$\n",
      "\n",
      "$\\mathbf{z_i}$ is a $K$-dimensional binary vector (features)\n",
      "\n",
      "$\\mathbf{A}$ is a $K \\times D$ matrix of weights, with prior $A \\sim \\text{Normal}(0,\\sigma_A^2 \\mathbf{I})$\n",
      "\n",
      "$Z \\sim \\text{IBP}(\\alpha)$, where $m_k$ is the number of objects with feature $k$\n",
      "\n",
      "$p(Z | \\alpha) = \\dfrac{\\alpha^K}{\\prod^{2^N-1}_{h=1}K_h!} \\exp(-\\alpha H_N) \\prod^{K}_{k=1}\\dfrac{(N-m_k)!(m_k-1)!}{N!}$\n",
      "\n",
      "Note 1: $\\alpha$ is a variable influencing the number of features  $D$\n",
      "\n",
      "Note 2: $K_h$ is the number of features with history $h$ (whether the $N$ images possess this feature, $2^N-1$ possibilities in total)\n",
      "\n",
      "Note 3: $H_N$ is the $N^{\\text{th}}$ harmonic number, and $m_k$ is the number of objects with feature $k$\n",
      "\n",
      "### Posterior Distribution Calculation\n",
      "\n",
      "Priors: $P(z_{ik}=1 | \\mathbf{z_{-i,k}}) = \\dfrac{m_{-i,k}+\\dfrac{\\alpha}{K}}{N+\\dfrac{\\alpha}{K}} \\rightarrow \\dfrac{m_{-i,k}}{N}$ as $K \\rightarrow \\infty$\n",
      "\n",
      "Likelihood: $\\mathbf{X}|(\\mathbf{Z},\\mathbf{A},\\mathbf{\\sigma_X}) \\sim \\text{Normal}(\\mathbf{ZA},\\Sigma_X = \\sigma_X^2\\mathbf{I})$\n",
      "\n",
      "i.e. $P(\\mathbf{X} | \\mathbf{Z}, \\sigma_X, \\sigma_A) = \\dfrac{1}{(2\\pi)^{ND/2} \\sigma_X^{(N-K)D} \\sigma_A^{KD} |\\mathbf{Z}^T\\mathbf{Z} + \\dfrac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I}|^{D/2}} \\exp\\{-\\dfrac{1}{2\\sigma^2_X} \\text{tr}(\\mathbf{X}^T(\\mathbf{I}-\\mathbf{Z}(\\mathbf{Z}^T\\mathbf{Z}+\\dfrac{\\sigma_X^2}{\\sigma_A^2}\\mathbf{I})^{-1}\\mathbf{Z}^T)\\mathbf{X})\\}$\n",
      "\n",
      "Note: The coding is tricky... need to use np.log to manage high powers\n",
      "\n",
      "Full conditional distribution: $P(z_{ik} | \\mathbf{X,Z_{-i,k}},\\sigma_X, \\sigma_A) \\propto P(\\mathbf{X} | \\mathbf{Z},\\sigma_X, \\sigma_A) P(z_{ik} | \\mathbf{z_{-i,k}})$\n",
      "\n",
      "### Gibbs Sampler\n",
      "\n",
      "Initialization: $\\sigma_A = 0.5, \\sigma_X = 1.7, \\alpha \\sim Ga(1,1)$\n",
      "\n",
      "1. Generate $\\alpha|Z \\sim Ga(1+K_+,1+\\sum^{N}_{i=1}H_i)$, where $K_+ = \\sum^{2^N-1}_{h=1}K_h$ and $K_+$ is the number of features for which $m_k > 0$\n",
      "2. Generate $Z|\\alpha \\sim IBP(\\alpha)$\n",
      "3. Sample $\\sigma_{X}^* = \\sigma_X + \\epsilon$, where $\\epsilon \\sim \\text{Unif}(-0.05,0.05)$, and accept $\\sigma_{X}^*$ by Metropolis (not just when the likelihood is larger, i.e. $P(\\sigma_{X}^*) > P(\\sigma_{X})$)\n",
      "4. Sample $\\sigma_{A}^* = \\sigma_A + \\epsilon$, where $\\epsilon \\sim \\text{Unif}(-0.05,0.05)$, and accept $\\sigma_{A}^*$ by Metropolis (not just when the likelihood is larger, i.e. $P(\\sigma_{A}^*) > P(\\sigma_{A})$)\n",
      "5. Sample $A \\sim \\text{Normal}(0,\\sigma_A^2 I)$\n",
      "6. Sample $X \\sim \\text{Normal}(ZA,\\sigma_X^2 I)$\n",
      "\n",
      "Metropolis for $\\sigma_A$ (for $\\sigma_X$ is similar):\n",
      "\n",
      "Current value: $\\sigma_{A}^{(n)}$\n",
      "\n",
      "Candidate value: $\\sigma_{A}^{*}$\n",
      "\n",
      "Generate $r \\sim \\text{Unif}(0,1)$\n",
      "\n",
      "Accept $\\sigma_{A}^{*}$ if $r < \\text{min}\\{ 1, \\dfrac{P(\\sigma_{A}^{*} | \\mathbf{Z,X},\\sigma_X)}{P(\\sigma_{A}^{(n)} | \\mathbf{Z,X},\\sigma_X)} \\}$\n",
      "\n",
      "# More notes for coding\n",
      "\n",
      "- The $i$th Harmonic number is $\\sum_{j=1}^{i}\\dfrac{1}{j}$\n",
      "\n",
      "The sum of the first $N$ Harmonic numbers is $\\sum_{i=1}^{N}H_i = N \\times \\dfrac{1}{i} + (N-1) \\times \\dfrac{1}{i+1} + \\cdots + 1 \\times \\dfrac{1}{i+(N-1)}$\n",
      "\n",
      "- $K_+ = \\sum^{2^N-1}_{h=1}K_h$ and $K_+$ is the number of features for which $m_k > 0$\n",
      "\n",
      "This can be obtained from the IBP (Indian Buffet Process) algorithm/function"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Algorithm Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Indian Buffet Process (IBP) Steps\n",
      "Probability must be between 0 and 1: $ 0 \\leq \\dfrac{m_{-i,k}+\\dfrac{\\alpha}{K}}{N+\\dfrac{\\alpha}{K}}, \\dfrac{m_{-i,k}}{N} \\leq 1$\n",
      "\n",
      "### Generating Images\n",
      "Print the base images for verification of readability\n",
      "\n",
      "### Gibbs Sampler\n",
      "When sampling $z_{ik}$, the acceptance rate should not be too low (e.g. 10%)\n",
      "\n",
      "Probability must be between 0 and 1: $0 \\leq P(\\mathbf{X} | \\mathbf{Z}, \\sigma_X, \\sigma_A) \\leq 1$\n",
      "\n",
      "Metropolis algorithm: Acceptance rate should be between 20% and 50%"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}